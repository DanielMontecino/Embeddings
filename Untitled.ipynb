{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.StaticDataloader import StaticDataloader\n",
    "from dataloader.FileDataloader import FileDataloader\n",
    "from TripletLoss import TripletLoss\n",
    "from keras import optimizers\n",
    "from utils import get_dirs, get_database, get_net_object\n",
    "from visualize_embeddings import visualize_embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters\n",
    "\n",
    "# Select the net type. \n",
    "#See ./models/Resnet.py and ./models/Base.py\n",
    "net = ['base', 'resnet50'][0]\n",
    "layer_limit = 163        # Layer from which you can train\n",
    "preprocess_unit = False  # If use the ResNet50's preprocess_unit function \n",
    "\n",
    "# Select the dataset to train with\n",
    "database = ['cifar10', 'mnist', 'fashion_mnist', 'skillup'][3]\n",
    "\n",
    "epochs = 150         # Epoch to train\n",
    "learn_rate = 0.001 # Initial learning rate\n",
    "patience = 25        # Number of epochs without improvement before stop the training\n",
    "\n",
    "\n",
    "# TripletLoss parameters\n",
    "ims_per_id = 4      # Number of images per class\n",
    "ids_per_batch = 10  # Number of classes per batch\n",
    "margin = 0.5        # Margin of TripletLoss \n",
    "squared = True      # If use Euclidean distance or square of euclidean distance\n",
    "semi_hard = False    # If use semi hard triplet loss or hard triplet loss\n",
    "\n",
    "\n",
    "# Parameters of the nets\n",
    "embedding_size = 128       # Usefull when a Dense layer is added at the end of the net\n",
    "data_augmentation = False  # If use data augmentation \n",
    "\n",
    "# built model's parameters\n",
    "dropout = 0.3            # Dropout probability of each layer. Conv layers use SpatialDropout2D \n",
    "blocks = 6               # Number of (Conv -> Act -> BN -> MaxPool -> Dropout) blocks\n",
    "n_channels = 16          # Number of channels (or feature maps) of the first convolution block.\n",
    "                         # the following ones are 1.5 times the number of channels of the previous block\n",
    "weight_decay = 1e-4 * 0  \n",
    "\n",
    "# dataloader parameters.\n",
    "# Folder's path where the files query.txt and bounding_box_train.txt are \n",
    "# query.txt contains the path and the class of test images\n",
    "# bounding_box_train.txt contains the path and the class of train images\n",
    "path = '/home/daniel/proyectos/product_detection/web_market_preproces/duke_from_images' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "folder = os.path.join(path, 'bounding_box_test')\n",
    "query = os.path.join(path, 'query')\n",
    "txt = os.path.join(path, 'query.txt')\n",
    "files = os.listdir(folder)\n",
    "query_files = os.listdir(query)\n",
    "print(len(files), len(query_files))\n",
    "\n",
    "with open(txt, 'w') as f:\n",
    "    for file in files:\n",
    "        clase = int(file.split('_')[0])\n",
    "        if clase in [5022, 5027]:\n",
    "            continue\n",
    "        if clase == 5026:\n",
    "            clase = 5025\n",
    "        f.write(os.path.join(path,'bounding_box_test', file) + \" \" + str(clase) + '\\n')\n",
    "    for file in query_files:\n",
    "        clase = int(file.split('_')[0])\n",
    "        f.write(os.path.join(path,'query', file) + \" \" + str(clase) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dir where to save models, logs, etc..\n",
    "exp_dir, log_dir, model_weights_path, model_name = get_dirs(database)\n",
    "print(exp_dir, log_dir, model_weights_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TripletLoss object. It contains the data generators\n",
    "tl_h = TripletLoss(ims_per_id, ids_per_batch, margin, squared)\n",
    "if semi_hard:\n",
    "    loss = tl_h.sm_loss\n",
    "else:\n",
    "    loss = tl_h.loss\n",
    "    \n",
    "opt = optimizers.Adam(lr=learn_rate)\n",
    "data, input_size = get_database(database) # if database == 'skillup'. data is None\n",
    "im_size = input_size[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args_train = dict(featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                               samplewise_center=False,  # set each sample mean to 0\n",
    "                               featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                               samplewise_std_normalization=False,  # divide each input by its std\n",
    "                               zca_whitening=False,  # apply ZCA whitening\n",
    "                               rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                               zoom_range=0.1,  # Randomly zoom image\n",
    "                               width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                               height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                               horizontal_flip=False,  # randomly flip images\n",
    "                               vertical_flip=False)\n",
    "if not data_augmentation:\n",
    "    data_gen_args_train = {}\n",
    "\n",
    "model_args = dict(embedding_dim=embedding_size,\n",
    "                  input_shape=input_size,\n",
    "                  drop=dropout,\n",
    "                  blocks=blocks,\n",
    "                  n_channels=n_channels,\n",
    "                  weight_decay=weight_decay,\n",
    "                  layer_limit=163,\n",
    "                  patience=patience)\n",
    "\n",
    "data_loader_args = dict(path=path,\n",
    "                        ims_per_id=ims_per_id,\n",
    "                        ids_per_batch=ids_per_batch,\n",
    "                        target_image_size=im_size,\n",
    "                        data_gen_args=data_gen_args_train,\n",
    "                        #preprocess_unit=False,\n",
    "                        data=data)\n",
    "\n",
    "if database == 'skillup':\n",
    "    dl = FileDataloader(preprocess_unit=preprocess_unit, **data_loader_args)\n",
    "else:\n",
    "    dl = StaticDataloader(**data_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To look at the images delivered by the Dataloader\n",
    "\n",
    "g = dl.triplet_train_generator() # Train generator\n",
    "v = dl.triplet_test_generator()  # Test generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(v)\n",
    "\n",
    "for id_ in range(ids_per_batch):\n",
    "    for k in range(ims_per_id):\n",
    "        plt.subplot(1, ims_per_id, k + 1)\n",
    "        im = a[0][k + id_ * ims_per_id]\n",
    "        label = a[1][k + id_ * ims_per_id]\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        plt.title( str(label))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "model = get_net_object(net, **model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model.compile(opt, loss)\n",
    "#model.train_generator(dl, model_weights_path, epochs, log_dir)\n",
    "for i in range(3):\n",
    "    model.train_generator(dl, model_weights_path, epochs,\n",
    "                          learn_rate/(np.sqrt(10)**i), log_dir)\n",
    "    model_path = model_weights_path.split('.')[0] + '_h_v%d.h5' % i\n",
    "    model.save_model(model_path)\n",
    "    print(\"Saved model %d in %s\" %(i, model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = model_weights_path.split('.')[0] + '_h_final.h5'\n",
    "model.save_model(model_path)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project the vectors with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context('notebook', font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def scatter(x, labels, subtitle=None, classes=10):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", classes))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(10, 10))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[labels.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "    for i in range(classes):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = np.median(x[labels == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=12)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "        \n",
    "    if subtitle != None:\n",
    "        plt.suptitle(subtitle)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def change_classes(y):\n",
    "    change_dict = {}\n",
    "    new_class = 0\n",
    "    new_labels = []\n",
    "    for label in y:\n",
    "        if label not in change_dict.keys():\n",
    "            change_dict[label] = new_class\n",
    "            new_class += 1\n",
    "        new_labels.append(change_dict[label])\n",
    "    n_classes = len(change_dict.keys())\n",
    "    return np.array(new_labels), n_classes\n",
    "\n",
    "# Getting a batch from training and validation data for visualization\n",
    "x_train_tsne = []; y_train_tsne = []\n",
    "x_val_tsne = []; y_val_tsne = []\n",
    "\n",
    "for _ in range(10):\n",
    "    x_train_tsne_, y_train_tsne_ = next(g)\n",
    "    x_val_tsne_, y_val_tsne_ = next(v)\n",
    "    x_train_tsne.append(x_train_tsne_)\n",
    "    y_train_tsne.append(y_train_tsne_)\n",
    "    x_val_tsne.append(x_val_tsne_)\n",
    "    y_val_tsne.append(y_val_tsne_)\n",
    "    \n",
    "x_train_tsne = np.concatenate(x_train_tsne, axis=0)\n",
    "x_val_tsne = np.concatenate(x_val_tsne, axis=0)\n",
    "y_train_tsne = np.concatenate(y_train_tsne, axis=0)\n",
    "y_val_tsne = np.concatenate(y_val_tsne, axis=0)\n",
    "\n",
    "print(x_train_tsne.shape, x_val_tsne.shape)\n",
    "print(y_train_tsne.shape)\n",
    "\n",
    "y_train_tsne, t_classes = change_classes(y_train_tsne)\n",
    "y_val_tsne, v_classes = change_classes(y_val_tsne)\n",
    "\n",
    "x_train_tsne = x_train_tsne.reshape(x_train_tsne.shape[0], -1)\n",
    "x_val_tsne = x_val_tsne.reshape(x_val_tsne.shape[0],-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating and visualizing t-SNE embeddings of the raw data\n",
    "tsne = TSNE()\n",
    "train_tsne_embeds = tsne.fit_transform(x_train_tsne)\n",
    "scatter(train_tsne_embeds, y_train_tsne, \"Samples from Training Data\", t_classes)\n",
    "\n",
    "\n",
    "\n",
    "eval_tsne_embeds = tsne.fit_transform(x_val_tsne)\n",
    "scatter(eval_tsne_embeds, y_val_tsne, \"Samples from Validation Data\", v_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_classes, v_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "embed_train = model.model.predict(x_train_tsne.reshape(-1, 224, 224, 3))\n",
    "embed_val =   model.model.predict(x_val_tsne.reshape(-1, 224, 224, 3))\n",
    "\n",
    "    \n",
    "# Generating and visualizing t-SNE embeddings of the vectors\n",
    "tsne = TSNE()\n",
    "train_tsne_embeds = tsne.fit_transform(embed_train)\n",
    "scatter(train_tsne_embeds, y_train_tsne, \"Samples from Training Data\", t_classes)\n",
    "\n",
    "\n",
    "\n",
    "eval_tsne_embeds = tsne.fit_transform(embed_val)\n",
    "scatter(eval_tsne_embeds, y_val_tsne, \"Samples from Validation Data\", v_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the class of each number\n",
    "\n",
    "for i in range(v_classes):\n",
    "    for k, l in enumerate(y_val_tsne):\n",
    "        if l == i:\n",
    "            plt.imshow(x_val_tsne[k].reshape(224,224,3))\n",
    "            plt.title(l)\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

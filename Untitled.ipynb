{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.StaticDataloader import StaticDataloader\n",
    "from dataloader.FileDataloader import FileDataloader\n",
    "from TripletLoss import TripletLoss\n",
    "from keras import optimizers\n",
    "from utils import get_dirs, get_database, get_net_object\n",
    "from visualize_embeddings import visualize_embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters\n",
    "\n",
    "# Select the net type. \n",
    "#See ./models/Resnet.py and ./models/Base.py\n",
    "net = ['base', 'resnet50'][1]\n",
    "layer_limit = 164        # Layer from which you can train\n",
    "preprocess = False if net == 'base' else True        # If use the ResNet50's preprocess_unit function \n",
    "\n",
    "# Select the dataset to train with\n",
    "database = ['cifar10', 'mnist', 'fashion_mnist', 'skillup'][3]\n",
    "\n",
    "epochs = 100         # Epoch to train\n",
    "learn_rate = 0.0001 # Initial learning rate\n",
    "patience = 20        # Number of epochs without improvement before stop the training\n",
    "\n",
    "\n",
    "# TripletLoss parameters\n",
    "ims_per_id = 4      # Number of images per class\n",
    "ids_per_batch = 32  # Number of classes per batch\n",
    "margin = 0.5        # Margin of TripletLoss \n",
    "squared = True      # If use Euclidean distance or square of euclidean distance\n",
    "semi_hard = True    # If use semi hard triplet loss or hard triplet loss\n",
    "\n",
    "\n",
    "# Parameters of the nets\n",
    "embedding_size = 128       # Usefull when a Dense layer is added at the end of the net\n",
    "data_augmentation = False  # If use data augmentation \n",
    "\n",
    "# built model's parameters\n",
    "dropout = 0.35            # Dropout probability of each layer. Conv layers use SpatialDropout2D \n",
    "blocks = 6                # Number of (Conv -> Act -> BN -> MaxPool -> Dropout) blocks\n",
    "n_channels = 16           # Number of channels (or feature maps) of the first convolution block.\n",
    "                          # the following ones are 1.5 times the number of channels of the previous block\n",
    "weight_decay = 1e-4 * 0  \n",
    "\n",
    "# dataloader parameters.\n",
    "# Folder's path where the files query.txt and bounding_box_train.txt are \n",
    "# query.txt contains the path and the class of test images\n",
    "# bounding_box_train.txt contains the path and the class of train images\n",
    "path = '/home/daniel/proyectos/product_detection/web_market_preproces/duke_from_images' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "folder = os.path.join(path, 'bounding_box_test')\n",
    "query = os.path.join(path, 'query')\n",
    "txt = os.path.join(path, 'query.txt')\n",
    "files = os.listdir(folder)\n",
    "query_files = os.listdir(query)\n",
    "print(len(files), len(query_files))\n",
    "\n",
    "with open(txt, 'w') as f:\n",
    "    for file in files:\n",
    "        clase = int(file.split('_')[0])\n",
    "        if clase in [5022, 5027]:\n",
    "            continue\n",
    "        f.write(os.path.join(path,'bounding_box_test', file) + \" \" + str(clase) + '\\n')\n",
    "    for file in query_files:\n",
    "        clase = int(file.split('_')[0])\n",
    "        f.write(os.path.join(path,'query', file) + \" \" + str(clase) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dir where to save models, logs, etc..\n",
    "exp_dir, log_dir, model_weights_path, model_name = get_dirs(database)\n",
    "print(exp_dir, log_dir, model_weights_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, input_size = get_database(database) # if database == 'skillup'. data is None\n",
    "im_size = input_size[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args_train = dict(featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                               samplewise_center=False,  # set each sample mean to 0\n",
    "                               featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                               samplewise_std_normalization=False,  # divide each input by its std\n",
    "                               zca_whitening=False,  # apply ZCA whitening\n",
    "                               rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                               zoom_range=0.1,  # Randomly zoom image\n",
    "                               width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                               height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                               horizontal_flip=False,  # randomly flip images\n",
    "                               vertical_flip=False)\n",
    "if not data_augmentation:\n",
    "    data_gen_args_train = {}\n",
    "\n",
    "model_args = dict(embedding_dim=embedding_size,\n",
    "                  input_shape=input_size,\n",
    "                  drop=dropout,\n",
    "                  blocks=blocks,\n",
    "                  n_channels=n_channels,\n",
    "                  weight_decay=weight_decay,\n",
    "                  layer_limit=layer_limit,\n",
    "                  patience=patience)\n",
    "\n",
    "data_loader_args = dict(path=path,\n",
    "                        ims_per_id=ims_per_id,\n",
    "                        ids_per_batch=ids_per_batch,\n",
    "                        target_image_size=im_size,\n",
    "                        data_gen_args=data_gen_args_train,\n",
    "                        pre_process_unit=preprocess,\n",
    "                        data=data)\n",
    "\n",
    "if database == 'skillup':\n",
    "    dl = FileDataloader(**data_loader_args)\n",
    "else:\n",
    "    dl = StaticDataloader(**data_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To look at the images delivered by the Dataloader\n",
    "\n",
    "g = dl.triplet_train_generator(train=True) # Train generator\n",
    "v = dl.triplet_test_generator(train=True)  # Test generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(v)\n",
    "\n",
    "for id_ in range(10):\n",
    "    for k in range(ims_per_id):\n",
    "        plt.subplot(1, ims_per_id, k + 1)\n",
    "        im = a[0][k + id_ * ims_per_id]\n",
    "        label = a[1][k + id_ * ims_per_id]\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        plt.title( str(label))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "keras.backend.clear_session()\n",
    "model = get_net_object(net, **model_args)\n",
    "\n",
    "# TripletLoss object. It contains the data generators\n",
    "tl_h = TripletLoss(ims_per_id, ids_per_batch, margin, squared)\n",
    "if semi_hard:\n",
    "    loss = tl_h.sm_loss\n",
    "else:\n",
    "    print(\"Hard loss\")\n",
    "    loss = tl_h.loss\n",
    "    \n",
    "opt = optimizers.Adam(lr=learn_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Lambda, Flatten\n",
    "import keras.backend as K\n",
    "\n",
    "def replace_av(mod, max_=True, idx=-2):\n",
    "    if max_:\n",
    "        m = MaxPooling2D()(mod.get_layer(index=-idx).output)\n",
    "    else:\n",
    "        m = AveragePooling2D()(mod.get_layer(index=-idx).output)\n",
    "    m = Flatten()(m)\n",
    "    #m = Lambda(lambda x: K.l2_normalize(x, axis=-1))(m)\n",
    "    mo = Model(mod.input, m)\n",
    "    return mo\n",
    "\n",
    "model.model = replace_av(model.model, max_=False, idx=4)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "for run in range(3):\n",
    "    lr = learn_rate/(np.sqrt(10)**run)\n",
    "    print(\"\\nLearning rate: %0.4f\" % lr)\n",
    "    model.compile(opt, loss)\n",
    "    model.train_generator(dl, model_weights_path, epochs,\n",
    "                          lr, log_dir)\n",
    "    model_path = model_weights_path.split('.')[0] + '_av_v%d.h5' % run\n",
    "    model.save_model(model_path)\n",
    "    print(\"Saved model %d in %s\" %(run, model_path))\n",
    "    \n",
    "    model.model = replace_av(model.model, max_=True, idx=3)\n",
    "    model_path = model_weights_path.split('.')[0] + '_max_v%d.h5' % run\n",
    "    model.save_model(model_path)\n",
    "    print(\"Saved model %d in %s\" %(run, model_path))\n",
    "    model.model = replace_av(model.model, max_=False, idx=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TripletLoss import normalize, pairwise_distance\n",
    "from keras.models import load_model\n",
    "\n",
    "class Sess:\n",
    "    def __init__(self, path, squared=False):\n",
    "        self.sess = K.get_session()\n",
    "        self.model_path = path\n",
    "        self.squared = squared\n",
    "        self.dists, self.feats = self.generate()\n",
    "        \n",
    "        \n",
    "    def generate(self):\n",
    "        self.model = load_model(self.model_path)\n",
    "        y_pred = normalize(self.model.output, axis=-1)\n",
    "        dist_mat = pairwise_distance(y_pred, squared=self.squared)\n",
    "        return dist_mat, self.model.output\n",
    "    \n",
    "    def calc(self, images):\n",
    "        [dists, feats] = self.sess.run(\n",
    "            [self.dists, self.feats],\n",
    "            feed_dict={\n",
    "                self.model.input: images,\n",
    "                K.learning_phase(): 0\n",
    "            })\n",
    "        return dists, feats\n",
    "\n",
    "\n",
    "s = Sess('exp/skillup/run_70//model_weights_max_v2.h5')\n",
    "d, f = s.calc(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = model_weights_path.split('.')[0] + '_h_final.h5'\n",
    "model.save_model(model_path)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project the vectors with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context('notebook', font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def scatter(x, labels, subtitle=None, classes=10):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", classes))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(10, 10))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[labels.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "    for i in range(classes):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = np.median(x[labels == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=12)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "        \n",
    "    if subtitle != None:\n",
    "        plt.suptitle(subtitle)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def change_classes(y):\n",
    "    change_dict = {}\n",
    "    new_class = 0\n",
    "    new_labels = []\n",
    "    for label in y:\n",
    "        if label not in change_dict.keys():\n",
    "            change_dict[label] = new_class\n",
    "            new_class += 1\n",
    "        new_labels.append(change_dict[label])\n",
    "    n_classes = len(change_dict.keys())\n",
    "    return np.array(new_labels), n_classes\n",
    "\n",
    "# Getting a batch from training and validation data for visualization\n",
    "x_train_tsne = []; y_train_tsne = []\n",
    "x_val_tsne = []; y_val_tsne = []\n",
    "\n",
    "for _ in range(10):\n",
    "    x_train_tsne_, y_train_tsne_ = next(g)\n",
    "    x_val_tsne_, y_val_tsne_ = next(v)\n",
    "    x_train_tsne.append(x_train_tsne_)\n",
    "    y_train_tsne.append(y_train_tsne_)\n",
    "    x_val_tsne.append(x_val_tsne_)\n",
    "    y_val_tsne.append(y_val_tsne_)\n",
    "    \n",
    "x_train_tsne = np.concatenate(x_train_tsne, axis=0)\n",
    "x_val_tsne = np.concatenate(x_val_tsne, axis=0)\n",
    "y_train_tsne = np.concatenate(y_train_tsne, axis=0)\n",
    "y_val_tsne = np.concatenate(y_val_tsne, axis=0)\n",
    "\n",
    "print(x_train_tsne.shape, x_val_tsne.shape)\n",
    "print(y_train_tsne.shape)\n",
    "\n",
    "y_train_tsne, t_classes = change_classes(y_train_tsne)\n",
    "y_val_tsne, v_classes = change_classes(y_val_tsne)\n",
    "\n",
    "x_train_tsne = x_train_tsne.reshape(x_train_tsne.shape[0], -1)\n",
    "x_val_tsne = x_val_tsne.reshape(x_val_tsne.shape[0],-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "embed_train = model.model.predict(x_train_tsne.reshape(-1, 224, 224, 3))\n",
    "embed_val =   model.model.predict(x_val_tsne.reshape(-1, 224, 224, 3))\n",
    "\n",
    "    \n",
    "# Generating and visualizing t-SNE embeddings of the vectors\n",
    "tsne = TSNE()\n",
    "train_tsne_embeds = tsne.fit_transform(embed_train)\n",
    "scatter(train_tsne_embeds, y_train_tsne, \"Samples from Training Data\", t_classes)\n",
    "\n",
    "\n",
    "\n",
    "eval_tsne_embeds = tsne.fit_transform(embed_val)\n",
    "scatter(eval_tsne_embeds, y_val_tsne, \"Samples from Validation Data\", v_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating and visualizing t-SNE embeddings of the raw data\n",
    "tsne = TSNE()\n",
    "train_tsne_embeds = tsne.fit_transform(x_train_tsne)\n",
    "scatter(train_tsne_embeds, y_train_tsne, \"Samples from Training Data\", t_classes)\n",
    "\n",
    "\n",
    "\n",
    "eval_tsne_embeds = tsne.fit_transform(x_val_tsne)\n",
    "scatter(eval_tsne_embeds, y_val_tsne, \"Samples from Validation Data\", v_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_classes, v_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the class of each number\n",
    "\n",
    "for i in range(v_classes):\n",
    "    for k, l in enumerate(y_val_tsne):\n",
    "        if l == i:\n",
    "            plt.imshow(x_val_tsne[k].reshape(224,224,3))\n",
    "            plt.title(l)\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "class LrTestFinderWithLinearDecaySchedule(keras.callbacks.Callback):\n",
    "    \"\"\"Linear decay with warmup learning rate scheduler\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate_base,\n",
    "                 total_steps,\n",
    "                 global_step_init=0,\n",
    "                 warmup_learning_rate=0.0,\n",
    "                 warmup_steps=0,\n",
    "                 hold_base_rate_steps=0,\n",
    "                 verbose=0):\n",
    "        super(LrTestFinderWithLinearDecaySchedule, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = global_step_init\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.hold_base_rate_steps = hold_base_rate_steps\n",
    "        self.verbose = verbose\n",
    "        self.learning_rates = []\n",
    "        self.max_lr = 0\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "        self.best_loss = 1e9\n",
    "        self.lr_step = (self.learning_rate_base / self.warmup_learning_rate) ** (1 / self.warmup_steps)\n",
    "        #self.lr_step = (self.learning_rate_base - self.warmup_learning_rate) * (1 / self.warmup_steps)\n",
    "        self.decreasing = False\n",
    "        self.steps_without_improve = 0\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        K.set_value(self.model.optimizer.lr, self.warmup_learning_rate)\n",
    "        self.decreasing = False\n",
    "        \n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        loss = logs['loss']\n",
    "        self.losses.append(loss)\n",
    "        self.learning_rates.append(lr)\n",
    "        self.global_step = self.global_step + 1         \n",
    "        if not self.decreasing:\n",
    "            # Check whether the loss got too large or NaN\n",
    "            if self.global_step>self.warmup_steps*0.5:\n",
    "                n = 10\n",
    "                last_accs = np.mean(self.losses[-2*n:-n])\n",
    "                loss = np.mean(self.losses[-n::])\n",
    "                cond = loss > last_accs*4\n",
    "                if cond or math.isnan(loss) or self.global_step>self.warmup_steps:\n",
    "                    self.decreasing = True\n",
    "                    self.max_lr = lr / 5\n",
    "                    self.warmup_steps = self.global_step\n",
    "                    print(\"\\nLearning rate fund %0.5f\" % self.max_lr)\n",
    "                    return\n",
    "\n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "\n",
    "            # Increase the learning rate for the next batch\n",
    "            lr *= self.lr_step\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if self.decreasing:\n",
    "            lr = linear_decay_with_warmup(global_step=self.global_step,\n",
    "                                          learning_rate_base=self.max_lr,\n",
    "                                          total_steps=self.total_steps,\n",
    "                                          warmup_learning_rate=0,\n",
    "                                          warmup_steps=self.warmup_steps,\n",
    "                                          hold_base_rate_steps=self.hold_base_rate_steps)\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "            if self.verbose > 0:\n",
    "                print('\\nBatch %05d: setting learning '\n",
    "                      'rate to %s.' % (self.global_step + 1, lr))\n",
    "        \n",
    "                       \n",
    "def linear_decay_with_warmup(global_step, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps,\n",
    "                             hold_base_rate_steps):\n",
    "    if total_steps < warmup_steps:\n",
    "        raise ValueError('total_steps must be larger or equal to '\n",
    "                         'warmup_steps.')\n",
    "    learning_rate = learning_rate_base * (1 - (global_step - warmup_steps - hold_base_rate_steps\n",
    "         ) / float(total_steps - warmup_steps - hold_base_rate_steps))\n",
    "\n",
    "    if hold_base_rate_steps > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps,\n",
    "                                 learning_rate, learning_rate_base)\n",
    "    if warmup_steps > 0:\n",
    "        if learning_rate_base < warmup_learning_rate:\n",
    "            raise ValueError('learning_rate_base must be larger or equal to '\n",
    "                             'warmup_learning_rate.')\n",
    "        slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n",
    "        warmup_rate = slope * global_step + warmup_learning_rate\n",
    "        learning_rate = np.where(global_step < warmup_steps, warmup_rate,\n",
    "                                 learning_rate)\n",
    "    return np.where(global_step > total_steps, 0.0, learning_rate)\n",
    "\n",
    "class LinearDecayScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Linear decay with warmup learning rate scheduler\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate_base,\n",
    "                 total_steps,\n",
    "                 global_step_init=0,\n",
    "                 warmup_learning_rate=0.0,\n",
    "                 warmup_steps=0,\n",
    "                 hold_base_rate_steps=0,\n",
    "                 verbose=0):\n",
    "        \"\"\"Constructor for linear decay with warmup learning rate scheduler.\n",
    "\n",
    "            Arguments:\n",
    "                learning_rate_base {float} -- base learning rate.\n",
    "                total_steps {int} -- total number of training steps.\n",
    "\n",
    "            Keyword Arguments:\n",
    "                global_step_init {int} -- initial global step, e.g. from previous checkpoint.\n",
    "                warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n",
    "                warmup_steps {int} -- number of warmup steps. (default: {0})\n",
    "                hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n",
    "                                            before decaying. (default: {0})\n",
    "                verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n",
    "                \"\"\"\n",
    "\n",
    "        super(LinearDecayScheduler, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = global_step_init\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.hold_base_rate_steps = hold_base_rate_steps\n",
    "        self.verbose = verbose\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.global_step = self.global_step + 1\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        loss = logs['loss']\n",
    "        self.losses.append(loss)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = linear_decay_with_warmup(global_step=self.global_step,\n",
    "                                      learning_rate_base=self.learning_rate_base,\n",
    "                                      total_steps=self.total_steps,\n",
    "                                      warmup_learning_rate=self.warmup_learning_rate,\n",
    "                                      warmup_steps=self.warmup_steps,\n",
    "                                      hold_base_rate_steps=self.hold_base_rate_steps)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nBatch %05d: setting learning '\n",
    "                  'rate to %s.' % (self.global_step + 1, lr))\n",
    "            \n",
    "def linear_decay_with_warmup(global_step, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps,\n",
    "                             hold_base_rate_steps):\n",
    "    \"\"\"Linear decay schedule with warm up period.\n",
    "\n",
    "\n",
    "    In this schedule, the learning rate grows linearly from warmup_learning_rate\n",
    "    to learning_rate_base for warmup_steps, then transitions to a linear decay\n",
    "    schedule.\n",
    "\n",
    "    Arguments:\n",
    "        global_step {int} -- global step.\n",
    "        learning_rate_base {float} -- base learning rate.\n",
    "        total_steps {int} -- total number of training steps.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n",
    "        warmup_steps {int} -- number of warmup steps. (default: {0})\n",
    "        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n",
    "                                    before decaying. (default: {0})\n",
    "    Returns:\n",
    "      a float representing learning rate.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if warmup_learning_rate is larger than learning_rate_base,\n",
    "        or if warmup_steps is larger than total_steps.\n",
    "    \"\"\"\n",
    "    if total_steps < warmup_steps:\n",
    "        raise ValueError('total_steps must be larger or equal to '\n",
    "                         'warmup_steps.')\n",
    "    learning_rate = learning_rate_base * (1 - (global_step - warmup_steps - hold_base_rate_steps\n",
    "         ) / float(total_steps - warmup_steps - hold_base_rate_steps))\n",
    "\n",
    "    if hold_base_rate_steps > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps,\n",
    "                                 learning_rate, learning_rate_base)\n",
    "    if warmup_steps > 0:\n",
    "        if learning_rate_base < warmup_learning_rate:\n",
    "            raise ValueError('learning_rate_base must be larger or equal to '\n",
    "                             'warmup_learning_rate.')\n",
    "        slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n",
    "        warmup_rate = slope * global_step + warmup_learning_rate\n",
    "        learning_rate = np.where(global_step < warmup_steps, warmup_rate,\n",
    "                                 learning_rate)\n",
    "    return np.where(global_step > total_steps, 0.0, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "# TripletLoss object. It contains the data generators\n",
    "tl_h = TripletLoss(ims_per_id, ids_per_batch, margin, squared)\n",
    "if semi_hard:\n",
    "    loss = tl_h.sm_loss\n",
    "else:\n",
    "    loss = tl_h.loss\n",
    "    \n",
    "opt = optimizers.Adam(lr=learn_rate)\n",
    "model = get_net_object(net, **model_args)\n",
    "\n",
    "epochs = 60\n",
    "w_epochs = 30\n",
    "total_steps = dl.get_train_steps() * epochs\n",
    "w_steps = dl.get_train_steps() * w_epochs\n",
    "\n",
    "callbacks = [LinearDecayScheduler(learning_rate_base=0.04,\n",
    "                                 total_steps=total_steps,\n",
    "                                 warmup_learning_rate=0.00003,\n",
    "                                 warmup_steps=w_steps)]\n",
    "\n",
    "model.compile(opt, loss)\n",
    "model.model.fit_generator(generator=dl.triplet_train_generator(),\n",
    "                                     steps_per_epoch=dl.get_train_steps(),\n",
    "                                     epochs=epochs,\n",
    "                                     verbose=1,\n",
    "                                     callbacks=callbacks,\n",
    "                                     validation_data=dl.triplet_test_generator(),\n",
    "                                     validation_steps=dl.get_test_steps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.plot(callbacks[0].learning_rates)\n",
    "#plt.ylim([0,0.05])\n",
    "#plt.xlim([0,600])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(callbacks[0].losses, label='loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.xlim([0,600])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "path = '/home/daniel/proyectos/product_detection/web_market_preproces/duke_from_images/query' \n",
    "new_path = path + '2'\n",
    "os.makedirs(new_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for im in imgs:\n",
    "    im_path = os.path.join(path, im)\n",
    "    new_im_path = os.path.join(new_path, im)\n",
    "    c = int(im_path.split('/')[-1].split('_')[0])\n",
    "    imcv = cv2.imread(im_path)\n",
    "    H, W, _ = imcv.shape     \n",
    "    if H > W and c == 5025:\n",
    "        new_im_path = new_im_path.split('5025')\n",
    "        new_im_path = new_im_path[0] + '5026' + new_im_path[1]\n",
    "    cv2.imwrite(new_im_path, imcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
